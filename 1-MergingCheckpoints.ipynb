{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63c6f26b4ad57447",
   "metadata": {},
   "source": [
    "# Merging Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb79c26abc66a74e",
   "metadata": {},
   "source": [
    "As you can see from the scripts included in this project, we ended up batching the comparisons between our keyword utterances ($k \\in K$) and our context utterances ($c \\in C$). Partially, this was to decrease the noise in the office where the tower is stored while running our tests.\n",
    "\n",
    "The following scripts are designed to stitch those pieces back together again, largely using the CEDA object/framework to do so."
   ]
  },
  {
   "cell_type": "code",
   "id": "d0956efba2d9a14b",
   "metadata": {},
   "source": [
    "from CEDA import ceda_model\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "22b58f0c8f1c1b91",
   "metadata": {},
   "source": [
    "CKPT_PATH = 'data/ckpts'\n",
    "RAW_PATH = 'data/raw'\n",
    "OUT_PATH = 'data/results'\n",
    "OUT_NAME = 'ceda-results.csv'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "15eedd03133b6e92",
   "metadata": {},
   "source": [
    "df = []"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3bfa56f293eeab79",
   "metadata": {},
   "source": [
    "mod = ceda_model()\n",
    "files = [os.path.join(CKPT_PATH, f) for f in os.listdir(CKPT_PATH)]\n",
    "\n",
    "sum_cols = ['nx', 'Hxy']\n",
    "\n",
    "for f in tqdm(files):\n",
    "    mod.load_from_checkpoint(f)\n",
    "    graph = mod.graph_df(residualize=False)\n",
    "    \n",
    "    meta_data_cols = [col for col in list(graph) if col not in sum_cols]\n",
    "    \n",
    "    # per article, sum H, n_x for all SOTU pieces.\n",
    "    for link in graph['link'].unique():\n",
    "        sub = graph.loc[\n",
    "            graph['link'].isin([link]) \n",
    "            & (graph['nx'] > 0) \n",
    "            & (graph['ny'] > 0)\n",
    "            & (graph['Hxy'] != -404.404)\n",
    "        ]\n",
    "        doc = sub[meta_data_cols].to_dict(orient='records')[0]\n",
    "        for col in sum_cols:\n",
    "            doc[col] = sub[col].sum()\n",
    "        df += [doc]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7a3574b492327c1a",
   "metadata": {},
   "source": [
    "df = pd.concat(df, ignore_index=True)\n",
    "print(df.shape)\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Operations to sanitize data",
   "id": "2dea810affb84d13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And some last checks.",
   "id": "f0e1d110f6b73aa2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.isna().sum()",
   "id": "b82c52f2db4e7dd8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a8f33437a0328b29",
   "metadata": {},
   "source": [
    "Let's also take a moment now and anonymize some of the data (and save our anonymization key locally)"
   ]
  },
  {
   "cell_type": "code",
   "id": "73dc0044a1e985f6",
   "metadata": {},
   "source": [
    "anonymize_columns = [['president'], ['link']]\n",
    "for cols in anonymize_columns:\n",
    "    values = np.unique(df[cols].values)\n",
    "    values = np.random.choice(values, size=(len(values),), replace=False)\n",
    "    \n",
    "    conversion = {val:i+1 for i,val in enumerate(values)}\n",
    "    \n",
    "    # save conversion dictionary\n",
    "    f = open(\n",
    "        os.path.join(\n",
    "            OUT_PATH, \n",
    "            cols[0].replace('x_', '').replace('y_', '')+'.json'\n",
    "        ), \n",
    "        'w'\n",
    "    )\n",
    "    f.write(json.dumps(conversion,indent=4))\n",
    "    f.close()\n",
    "    \n",
    "    # anonymize the column\n",
    "    for col in cols:\n",
    "        print(col)\n",
    "        df[col] = [conversion[val] for val in tqdm(df[col].values)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "90365ec1e3d989f6",
   "metadata": {},
   "source": [
    "Finishing this, let's save the data."
   ]
  },
  {
   "cell_type": "code",
   "id": "ace17b546b6890c3",
   "metadata": {},
   "source": [
    "df.to_csv(os.path.join(OUT_PATH, OUT_NAME), index=False, encoding='utf-8')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.shape",
   "id": "686336084fb3e06a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "db121aa11950f572"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
